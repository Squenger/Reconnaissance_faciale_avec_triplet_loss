{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76621aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "#On prioritise l'utilisation du GPU si disponible\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw=torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data_raw=torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a96879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TripletFashionMNIST(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset=dataset\n",
    "        self.train_labels = self.dataset.targets #les labels des images\n",
    "        self.train_data = self.dataset.data #les images\n",
    "        self.labels_set = set(self.train_labels.numpy())# ensemble des labels uniques\n",
    "        self.label_to_indices = {label: np.where(self.train_labels.numpy() == label)[0]\n",
    "                                 for label in self.labels_set} # dictionnaire label: indices des images avec ce label\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        #Anchor (référence)\n",
    "        img1, label1 = self.dataset[index]\n",
    "            \n",
    "            #Positif avec le meme label\n",
    "        positive_index = index\n",
    "        while positive_index == index: #pour eviter de prendre la meme image\n",
    "            positive_index = np.random.choice(self.label_to_indices[label1])\n",
    "        img2, _ = self.dataset[positive_index]\n",
    "            \n",
    "            #Negatif avec un label different\n",
    "        negative_label = np.random.choice(list(self.labels_set - set([label1]))) # choix d'un label different\n",
    "        negative_index = np.random.choice(self.label_to_indices[negative_label]) #On prend une image avec ce label\n",
    "        img3, _ = self.dataset[negative_index]\n",
    "    \n",
    "        return (img1, img2, img3) ,[] #le label n'est pas utilise dans le triplet loss\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.dataset) #nombre d'images dans le dataset\n",
    "    \n",
    "train_dataset = TripletFashionMNIST(train_data_raw)\n",
    "train=DataLoader(train_dataset, batch_size=64, shuffle=True) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8306cd35",
   "metadata": {},
   "source": [
    "Architechture CNN: l'objectif n'est pas de classer directement mais de compresser l'image pour en faire un vecteur de nombres (embedding) sur lequel on va pouvoir faire des opérations.\n",
    "\n",
    "image de 28x28 pixels --> vecteur de 128 valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b542a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        # Réseau de neurones convolutifs\n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5), #premiere couche convolutionnelle\n",
    "            nn.ReLU(), #fonction d'activation non lineaire\n",
    "            nn.MaxPool2d(2, 2), #couche de pooling pour reduire la taille\n",
    "            nn.Conv2d(32, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        # Couche entièrement connectée\n",
    "        self.fc = nn.Sequential( \n",
    "            nn.Linear(64 * 4 * 4, 256), #réseau linéaire de taille adaptée à la sortie des convolutions\n",
    "            nn.PReLU(), #fonction d'activation non lineaire\n",
    "            nn.Linear(256, 256) ,\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(256, 128) #réseau linéaire de sortie de taille 128\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convnet(x) #passage par les couches convolutives, on obtient un cube de caractéristiques\n",
    "        x = x.view(x.size()[0], -1) #aplatir la sortie pour la couche entièrement connectée\n",
    "        x = self.fc(x) #passage par les couches entièrement connectées\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b44d0",
   "metadata": {},
   "source": [
    "Maintenant nous pouvons mettre en place le réseau Siamois "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1fb554c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamois(nn.Module):\n",
    "    def __init__(self, embedding=None):\n",
    "        super(Siamois, self).__init__()\n",
    "        if embedding is None:\n",
    "            self.embedding = Embedding() #utilisation du réseau d'embedding défini par défaut\n",
    "        else:\n",
    "            self.embedding = embedding  #utilisation de l'embedding fourni en paramètre (utile pour le transfert d'apprentissage )\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        #On passe les trois images (ancre, positif, négatif) dans le même réseau d'embedding\n",
    "        output1 = self.embedding(x1) #obtenir l'embedding de l'ancre\n",
    "        output2 = self.embedding(x2) #obtenir l'embedding du positif\n",
    "        output3 = self.embedding(x3) #obtenir l'embedding du négatif\n",
    "        return output1, output2, output3\n",
    "    \n",
    "embedding=Embedding()\n",
    "model = Siamois().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c15962",
   "metadata": {},
   "source": [
    "# Formule mathématique de la Triplet Loss\n",
    "\n",
    "On veut que la distance d(ancre,positif) soit le plus petit possible tout en assurant que d(ancre,negatif) soit grande. En ajoutant une marge cette condition se traduit par la formule : \n",
    "\n",
    "$$ \\Delta(A,P) - \\Delta(A,N) +marge <0$$\n",
    "\n",
    "Si cette condition est vérifié, la fonction de perte doit renvoyer 0 (car on est en dehor de la marge (comparaison avec les SVM)) Sinon la loss est positive et correspond à la valeur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b120d3dc",
   "metadata": {},
   "source": [
    "On définit alors une fonction de perte qui suit cette logique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e009a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, marge=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.marge = marge\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        # Calcul des distances euclidiennes \n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)  # Distance entre ancre et positif\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)  # Distance entre ancre et négatif\n",
    "        \n",
    "        # Calcul de la perte triplet\n",
    "        losses = torch.relu(distance_positive - distance_negative + self.marge) #relu vaut 0 si l'argument est négatif et l'argument sinon\n",
    "        return losses.mean()\n",
    "    \n",
    "\n",
    "critere = TripletLoss(marge=1.0).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5691cbe1",
   "metadata": {},
   "source": [
    "Lancement de la boucle d'entrainement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "925ed428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 0] loss: 0.997\n",
      "[Epoch 1, Batch 100] loss: 0.059\n",
      "[Epoch 1, Batch 200] loss: 0.258\n",
      "[Epoch 1, Batch 300] loss: 0.216\n",
      "[Epoch 1, Batch 400] loss: 0.121\n",
      "[Epoch 1, Batch 500] loss: 0.184\n",
      "[Epoch 1, Batch 600] loss: 0.174\n",
      "[Epoch 1, Batch 700] loss: 0.148\n",
      "[Epoch 1, Batch 800] loss: 0.074\n",
      "[Epoch 1, Batch 900] loss: 0.099\n",
      "Epoch 1 loss: 0.177\n",
      "[Epoch 2, Batch 0] loss: 0.133\n",
      "[Epoch 2, Batch 100] loss: 0.159\n",
      "[Epoch 2, Batch 200] loss: 0.166\n",
      "[Epoch 2, Batch 300] loss: 0.154\n",
      "[Epoch 2, Batch 400] loss: 0.063\n",
      "[Epoch 2, Batch 500] loss: 0.080\n",
      "[Epoch 2, Batch 600] loss: 0.144\n",
      "[Epoch 2, Batch 700] loss: 0.139\n",
      "[Epoch 2, Batch 800] loss: 0.070\n",
      "[Epoch 2, Batch 900] loss: 0.082\n",
      "Epoch 2 loss: 0.121\n",
      "[Epoch 3, Batch 0] loss: 0.118\n",
      "[Epoch 3, Batch 100] loss: 0.061\n",
      "[Epoch 3, Batch 200] loss: 0.065\n",
      "[Epoch 3, Batch 300] loss: 0.188\n",
      "[Epoch 3, Batch 400] loss: 0.078\n",
      "[Epoch 3, Batch 500] loss: 0.141\n",
      "[Epoch 3, Batch 600] loss: 0.033\n",
      "[Epoch 3, Batch 700] loss: 0.048\n",
      "[Epoch 3, Batch 800] loss: 0.167\n",
      "[Epoch 3, Batch 900] loss: 0.152\n",
      "Epoch 3 loss: 0.101\n",
      "[Epoch 4, Batch 0] loss: 0.094\n",
      "[Epoch 4, Batch 100] loss: 0.113\n",
      "[Epoch 4, Batch 200] loss: 0.098\n",
      "[Epoch 4, Batch 300] loss: 0.083\n",
      "[Epoch 4, Batch 400] loss: 0.096\n",
      "[Epoch 4, Batch 500] loss: 0.038\n",
      "[Epoch 4, Batch 600] loss: 0.172\n",
      "[Epoch 4, Batch 700] loss: 0.065\n",
      "[Epoch 4, Batch 800] loss: 0.050\n",
      "[Epoch 4, Batch 900] loss: 0.071\n",
      "Epoch 4 loss: 0.090\n",
      "[Epoch 5, Batch 0] loss: 0.222\n",
      "[Epoch 5, Batch 100] loss: 0.066\n",
      "[Epoch 5, Batch 200] loss: 0.069\n",
      "[Epoch 5, Batch 300] loss: 0.089\n",
      "[Epoch 5, Batch 400] loss: 0.059\n",
      "[Epoch 5, Batch 500] loss: 0.051\n",
      "[Epoch 5, Batch 600] loss: 0.076\n",
      "[Epoch 5, Batch 700] loss: 0.108\n",
      "[Epoch 5, Batch 800] loss: 0.037\n",
      "[Epoch 5, Batch 900] loss: 0.107\n",
      "Epoch 5 loss: 0.083\n",
      "[Epoch 6, Batch 0] loss: 0.025\n",
      "[Epoch 6, Batch 100] loss: 0.078\n",
      "[Epoch 6, Batch 200] loss: 0.059\n",
      "[Epoch 6, Batch 300] loss: 0.018\n",
      "[Epoch 6, Batch 400] loss: 0.115\n",
      "[Epoch 6, Batch 500] loss: 0.075\n",
      "[Epoch 6, Batch 600] loss: 0.119\n",
      "[Epoch 6, Batch 700] loss: 0.054\n",
      "[Epoch 6, Batch 800] loss: 0.043\n",
      "[Epoch 6, Batch 900] loss: 0.086\n",
      "Epoch 6 loss: 0.075\n",
      "[Epoch 7, Batch 0] loss: 0.044\n",
      "[Epoch 7, Batch 100] loss: 0.067\n",
      "[Epoch 7, Batch 200] loss: 0.020\n",
      "[Epoch 7, Batch 300] loss: 0.081\n",
      "[Epoch 7, Batch 400] loss: 0.038\n",
      "[Epoch 7, Batch 500] loss: 0.097\n",
      "[Epoch 7, Batch 600] loss: 0.026\n",
      "[Epoch 7, Batch 700] loss: 0.077\n",
      "[Epoch 7, Batch 800] loss: 0.048\n",
      "[Epoch 7, Batch 900] loss: 0.126\n",
      "Epoch 7 loss: 0.073\n",
      "[Epoch 8, Batch 0] loss: 0.049\n",
      "[Epoch 8, Batch 100] loss: 0.148\n",
      "[Epoch 8, Batch 200] loss: 0.116\n",
      "[Epoch 8, Batch 300] loss: 0.196\n",
      "[Epoch 8, Batch 400] loss: 0.092\n",
      "[Epoch 8, Batch 500] loss: 0.060\n",
      "[Epoch 8, Batch 600] loss: 0.020\n",
      "[Epoch 8, Batch 700] loss: 0.042\n",
      "[Epoch 8, Batch 800] loss: 0.076\n",
      "[Epoch 8, Batch 900] loss: 0.170\n",
      "Epoch 8 loss: 0.069\n",
      "[Epoch 9, Batch 0] loss: 0.089\n",
      "[Epoch 9, Batch 100] loss: 0.060\n",
      "[Epoch 9, Batch 200] loss: 0.011\n",
      "[Epoch 9, Batch 300] loss: 0.007\n",
      "[Epoch 9, Batch 400] loss: 0.018\n",
      "[Epoch 9, Batch 500] loss: 0.072\n",
      "[Epoch 9, Batch 600] loss: 0.048\n",
      "[Epoch 9, Batch 700] loss: 0.018\n",
      "[Epoch 9, Batch 800] loss: 0.016\n",
      "[Epoch 9, Batch 900] loss: 0.061\n",
      "Epoch 9 loss: 0.067\n",
      "[Epoch 10, Batch 0] loss: 0.034\n",
      "[Epoch 10, Batch 100] loss: 0.112\n",
      "[Epoch 10, Batch 200] loss: 0.143\n",
      "[Epoch 10, Batch 300] loss: 0.073\n",
      "[Epoch 10, Batch 400] loss: 0.053\n",
      "[Epoch 10, Batch 500] loss: 0.130\n",
      "[Epoch 10, Batch 600] loss: 0.119\n",
      "[Epoch 10, Batch 700] loss: 0.036\n",
      "[Epoch 10, Batch 800] loss: 0.081\n",
      "[Epoch 10, Batch 900] loss: 0.045\n",
      "Epoch 10 loss: 0.064\n"
     ]
    }
   ],
   "source": [
    "epoches = 10\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    running_loss = 0.0 #perte cumulée pour l'affichage\n",
    "    \n",
    "    for batch_index, (data,_) in enumerate(train):\n",
    "        (anchor, positive, negative)= data\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)#envoi des images au device (GPU)\n",
    "        \n",
    "        #remise à zero des gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        anchor_out, positive_out, negative_out = model(anchor, positive, negative) #passage des images dans le réseau\n",
    "        \n",
    "         # Calcul de la perte\n",
    "        \n",
    "        loss = critere(anchor_out, positive_out, negative_out)\n",
    "        loss.backward() #calcul des gradients backward utilise la fonction forward modifiée\n",
    "        optimizer.step() #mise à jour des poids du modèle\n",
    "        \n",
    "         # Affichage des statistiques\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if batch_index % 100 == 0:    # Afficher toutes les 100 mini-batches\n",
    "            print(f'[Epoch {epoch + 1}, Batch {batch_index }] loss: {loss.item() :.3f}')\n",
    "    print(f'Epoch {epoch + 1} loss: {running_loss / len(train):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ef8e77",
   "metadata": {},
   "source": [
    "#Test du fonctionnement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cda6378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance entre A et B (Différents) : 0.0141\n",
      "Distance entre A et A (Mêmes)      : 0.0000\n",
      "Le modèle distingue bien les images !\n"
     ]
    }
   ],
   "source": [
    "def tester_distance(img1, img2, model):\n",
    "    model.eval() # Mode évaluation\n",
    "    with torch.no_grad(): \n",
    "        img1 = img1.unsqueeze(0).to(device)\n",
    "        img2 = img2.unsqueeze(0).to(device)\n",
    "        \n",
    "        # On calcule les embeddings via le réseau d'embedding (pas le siamois complet)\n",
    "        emb1 = embedding(img1)\n",
    "        emb2 = embedding(img2)\n",
    "        \n",
    "        # Distance euclidienne\n",
    "        dist = (emb1 - emb2).pow(2).sum(1).item()\n",
    "        return dist\n",
    "\n",
    "# Testons !\n",
    "index_1 = 0 # Une botte (dans FashionMNIST test)\n",
    "index_2 = 2 # Un pull\n",
    "index_3 = 0 \n",
    "\n",
    "img_a, label_a = test_data_raw[index_1] # Image A\n",
    "img_b, label_b = test_data_raw[index_2] # Image B (Différente)\n",
    "img_c, label_c = test_data_raw[index_1] # Image C (Identique à A)\n",
    "\n",
    "# Affichage des images\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(1,2,1); plt.imshow(img_a.squeeze(), cmap='gray'); plt.title(\"Image A\")\n",
    "plt.subplot(1,2,2); plt.imshow(img_b.squeeze(), cmap='gray'); plt.title(\"Image B\")\n",
    "plt.show()\n",
    "\n",
    "dist_diff = tester_distance(img_a, img_b, model)\n",
    "dist_same = tester_distance(img_a, img_c, model)\n",
    "\n",
    "print(f\"Distance entre A et B (Différents) : {dist_diff:.4f}\")\n",
    "print(f\"Distance entre A et A (Mêmes)      : {dist_same:.4f}\")\n",
    "\n",
    "if dist_diff > dist_same:\n",
    "    print(\"Le modèle distingue bien les images !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea593063",
   "metadata": {},
   "source": [
    "Adaptation à la reconnaissance de visage.\n",
    "Les difficulté : \n",
    "* Il faut un plus grand dataset\n",
    "* Dans les dataset on a pas toujours plusieurs images de la meme personne\n",
    "* Les images ne sont pas forcement en noir et blanc et aussi sont beacoup plus volumineuses\n",
    "* C'est impossible d'entrainer le réseau complet sur un PC classique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb208b21",
   "metadata": {},
   "source": [
    "Nous allons faire d'abord faire une classe qui télécharge LFW (la base de donnée), filtre pour garder les personnes ayant au moins 2 photos, et enfin génère les triplet comme précédement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6192898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d31aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_lfw=transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) #Normalisation\n",
    "])\n",
    "lfw_raw = ImageFolder(root='data/archive/lfw-deepfunneled', transform=transform_lfw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644c75b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'images utilisables : 13233\n"
     ]
    }
   ],
   "source": [
    "class TripletLFW(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.dataset=dataset\n",
    "        \n",
    "        self.train_labels = [item[1] for item in self.dataset.imgs] #les labels des images\n",
    "        \n",
    "        self.labels_set = set(self.train_labels)# ensemble des labels uniques\n",
    "        self.label_to_indices = {label: np.where(np.array(self.train_labels) == label)[0]\n",
    "                                 for label in self.labels_set} # dictionnaire label: indices des images avec ce label\n",
    "        \n",
    "        #filtreage \n",
    "        self.valid_labels = [label for label in self.labels_set if len(self.label_to_indices[label]) >1]\n",
    "        \n",
    "        self.valid_indices = []\n",
    "        for label in self.valid_labels:\n",
    "            self.valid_indices.extend(self.label_to_indices[label])\n",
    "            \n",
    "            \n",
    "        print (f\"Nombre d'images utilisables : {len(self.valid_indices)}\")\n",
    "    def __getitem__(self, index):\n",
    "        #Anchor (référence)\n",
    "        real_index = self.valid_indices[index]\n",
    "        img1, label1 = self.dataset[real_index]\n",
    "            \n",
    "            #Positif avec le meme label\n",
    "        positive_index = real_index\n",
    "        tentative=0\n",
    "        while positive_index == real_index: #pour eviter de prendre la meme image\n",
    "            positive_index = np.random.choice(self.label_to_indices[label1])\n",
    "            tentative+=1\n",
    "            if tentative>100:\n",
    "                break\n",
    "        img2, _ = self.dataset[positive_index]\n",
    "            \n",
    "            #Negatif avec un label different - utiliser uniquement les labels valides\n",
    "        negative_label = np.random.choice(self.valid_labels)\n",
    "        \n",
    "        tentative=0\n",
    "        while negative_label == label1:  # S'assurer que le label négatif est différent\n",
    "            negative_label = np.random.choice(self.valid_labels)\n",
    "            \n",
    "            tentative +=1\n",
    "            if tentative>100:\n",
    "                break\n",
    "        negative_index = np.random.choice(self.label_to_indices[negative_label]) #On prend une image avec ce label\n",
    "        img3, _ = self.dataset[negative_index]\n",
    "    \n",
    "        return (img1, img2, img3) ,[] #le label n'est pas utilise dans le triplet loss\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.valid_indices) #nombre d'images valides dans le dataset\n",
    "    \n",
    "triplet_lfw_dataset = TripletLFW(lfw_raw)\n",
    "triplet_lfw_loader=DataLoader(triplet_lfw_dataset, batch_size=32, shuffle=True, num_workers=0) #batch avec 32 triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75eab0c",
   "metadata": {},
   "source": [
    "# Transfert Learning:\n",
    "Comme il sera très dur d'entrainer notre modèle, nous allons charger ResNet18 puis freez le réseau et on remplace la dernière couche par une nouvelle couche non-entrainée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ee553141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eacf5730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de paramètres à entraîner : 5\n"
     ]
    }
   ],
   "source": [
    "class FaceNetResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FaceNetResNet, self).__init__()\n",
    "        # Utilisation d'un ResNet18 pré-entraîné\n",
    "\n",
    "        self.resnet=models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False  # On gèle les poids du ResNet pré-entraîné\n",
    "\n",
    "\n",
    "        # Remplacement de la couche finale pour obtenir un embedding de taille 128\n",
    "        num_features = self.resnet.fc.in_features # Nombre de caractéristiques en entrée de la couche finale\n",
    "        \n",
    "        # Nouvelle couche finale dont la sortie est de taille 128\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "        # S'assurer que les nouvelles couches sont entraînables\n",
    "        for param in self.resnet.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "    \n",
    "embedding_resnet=FaceNetResNet()\n",
    "model_resnet = Siamois(embedding_resnet).to(device) #on réutilise la classe Siamois définie précédemment mais avec le nouvel embedding pré-entraîné\n",
    "\n",
    "para_a_modifier=[]\n",
    "for name, param in model_resnet.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        para_a_modifier.append(param)\n",
    "print(f\"Nombre de paramètres à entraîner : {len(para_a_modifier)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b6d75bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "critere_resnet = TripletLoss(marge=1.0).to(device)\n",
    "optimizer_resnet = optim.Adam(model_resnet.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9293ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 0] loss: 1.507\n",
      "[Epoch 1, Batch 100] loss: 1.138\n",
      "[Epoch 1, Batch 200] loss: 0.952\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#remise à zero des gradients\u001b[39;00m\n\u001b[32m     11\u001b[39m optimizer_resnet.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m anchor_out, positive_out, negative_out = \u001b[43mmodel_resnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#passage des images dans le réseau\u001b[39;00m\n\u001b[32m     15\u001b[39m  \u001b[38;5;66;03m# Calcul de la perte\u001b[39;00m\n\u001b[32m     17\u001b[39m loss = critere_resnet(anchor_out, positive_out, negative_out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mSiamois.forward\u001b[39m\u001b[34m(self, x1, x2, x3)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2, x3):\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m#On passe les trois images (ancre, positif, négatif) dans le même réseau d'embedding\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     output1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#obtenir l'embedding de l'ancre\u001b[39;00m\n\u001b[32m     12\u001b[39m     output2 = \u001b[38;5;28mself\u001b[39m.embedding(x2) \u001b[38;5;66;03m#obtenir l'embedding du positif\u001b[39;00m\n\u001b[32m     13\u001b[39m     output3 = \u001b[38;5;28mself\u001b[39m.embedding(x3) \u001b[38;5;66;03m#obtenir l'embedding du négatif\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mFaceNetResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torchvision/models/resnet.py:285\u001b[39m, in \u001b[36mResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torchvision/models/resnet.py:275\u001b[39m, in \u001b[36mResNet._forward_impl\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    273\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer1(x)\n\u001b[32m    274\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer2(x)\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer4(x)\n\u001b[32m    278\u001b[39m x = \u001b[38;5;28mself\u001b[39m.avgpool(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/container.py:253\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torchvision/models/resnet.py:92\u001b[39m, in \u001b[36mBasicBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m     90\u001b[39m     identity = x\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.bn1(out)\n\u001b[32m     94\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.relu(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/conv.py:553\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    537\u001b[39m         F.pad(\n\u001b[32m    538\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    545\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    546\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "n_epoches = 3\n",
    "model_resnet.train()\n",
    "\n",
    "for epoch in range(n_epoches):\n",
    "    running_loss = 0.0 #perte cumulée pour l'affichage\n",
    "    for batch_index, (data,_) in enumerate(triplet_lfw_loader):\n",
    "        (anchor, positive, negative)= data\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)#envoi des images au device (GPU)\n",
    "        \n",
    "        #remise à zero des gradients\n",
    "        optimizer_resnet.zero_grad()\n",
    "        \n",
    "        anchor_out, positive_out, negative_out = model_resnet(anchor, positive, negative) #passage des images dans le réseau\n",
    "        \n",
    "         # Calcul de la perte\n",
    "        \n",
    "        loss = critere_resnet(anchor_out, positive_out, negative_out)\n",
    "        loss.backward() #calcul des gradients backward utilise la fonction forward modifiée\n",
    "        optimizer_resnet.step() #mise à jour des poids du modèle\n",
    "        \n",
    "         # Affichage des statistiques\n",
    "        l=loss.item()\n",
    "        running_loss += l\n",
    "        if batch_index % 100 == 0:    # Afficher toutes les 100 mini-batches\n",
    "            print(f'[Epoch {epoch + 1}, Batch {batch_index }] loss: {l :.3f}')\n",
    "    print(f'Epoch {epoch + 1} loss: {running_loss / len(triplet_lfw_loader):.3f}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44d8c04",
   "metadata": {},
   "source": [
    "# Test et affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bd7e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormaliser(tensor):\n",
    "    tensor = tensor.clone()  # Crée une copie pour éviter de modifiwither l'original\n",
    "    #Inversion de la normalisation appliquée lors du pré-traitement\n",
    "    mean = torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1) \n",
    "    std = torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1)\n",
    "    tensor = tensor * std + mean\n",
    "    tensor = torch.clamp(tensor, 0, 1)  # S'assure que les valeurs sont entre 0 et 1\n",
    "    return tensor.permute(1, 2, 0).cpu().numpy()# Convertit le tenseur en format numpy pour affichage avec matplotlib\n",
    "\n",
    "def evaluer_paire(img1,img2,model,threshold=1.0):\n",
    "    model.eval()\n",
    "    \n",
    "    img1_batch=img1.unsqueeze(0).to(device)\n",
    "    img2_batch=img2.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        emb1=model.embedding_resnet(img1_batch)\n",
    "        emb1=model.embedding_resnet(img2_batch)\n",
    "        \n",
    "        \n",
    "        distance = (emb1-emb2).pow(2).sum(1).sqrt().item()\n",
    "        \n",
    "        \n",
    "    meme = distance<threshold\n",
    "    \n",
    "    color =\"green\" if meme else \"red\"\n",
    "    \n",
    "    decision= \"même personne\" if meme else \"personnes différentes\"\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.subplot(1,2)\n",
    "    \n",
    "    plt.imshow(denormaliser(img1))\n",
    "    plt.set_title(\"Image 1\")\n",
    "    \n",
    "    \n",
    "    plt.subplot(2,2)\n",
    "    plt.imshow(denormaliser(img2))\n",
    "    plt.set_title(\"Image 2\")\n",
    "    \n",
    "    plt.title(f\"Distance : {distance:.4f}\\nSeuil : {threshold} -> Verdict : {decisioon}\", \n",
    "                 color=color)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3647cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#meme personnes\n",
    "\n",
    "label_idx = random.choice(triplet_lfw_dataset.valid_labels) # On prend une personne au hasard\n",
    "indices = triplet_lfw_dataset.label_to_indices[label_idx]\n",
    "\n",
    "# On prend 2 photos différentes de cette personne\n",
    "idx1, idx2 = random.sample(indices, 2)\n",
    "\n",
    "\n",
    "img_same_1, _ = lfw_raw[idx1]\n",
    "img_same_2, _ = lfw_raw[idx2]\n",
    "\n",
    "print(\"TEST 1 : Devrait être la MÊME personne\")\n",
    "evaluer_paire(img_same_1, img_same_2, model, threshold=0.8)\n",
    "\n",
    "\n",
    "# On prend deux personnes différentes\n",
    "label_a = random.choice(triplet_lfw_dataset.valid_labels)\n",
    "label_b = random.choice(triplet_lfw_dataset.valid_labels)\n",
    "while label_a == label_b:\n",
    "    label_b = random.choice(triplet_lfw_dataset.valid_labels)\n",
    "# On prend une photo de chacun\n",
    "idx_a = random.choice(triplet_lfw_dataset.label_to_indices[label_a])\n",
    "idx_b = random.choice(triplet_lfw_dataset.label_to_indices[label_b])\n",
    "\n",
    "img_diff_1, _ = lfw_raw[idx_a]\n",
    "img_diff_2, _ = lfw_raw[idx_b]\n",
    "\n",
    "print(\"TEST 2 : Devrait être des personnes DIFFÉRENTES\")\n",
    "evaluer_paire(img_diff_1, img_diff_2, model, threshold=0.8)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
